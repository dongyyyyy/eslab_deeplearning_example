{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a097b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import model as NN\n",
    "import dataloader as DL\n",
    "import loss_function as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ae08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data file\n",
    "\n",
    "infile = open('../dataset/mnist.pkl','rb')\n",
    "mnist_data = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c027b3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Xtest', 'ytest', 'Xtrain', 'ytrain'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d50c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 20\n",
    "\n",
    "train_loader = DL.dataLoader(mnist_data['Xtrain'], mnist_data['ytrain'], batchsize)\n",
    "test_loader = DL.dataLoader(mnist_data['Xtest'], mnist_data['ytest'], batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2e83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN.Network(400, 10, [50, 20, 30 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th training loss = 2.300588310688372\n",
      "1th training accuracy = [0.13625]\n",
      "2th training loss = 2.28508780857587\n",
      "2th training accuracy = [0.17125]\n",
      "3th training loss = 2.242534100477234\n",
      "3th training accuracy = [0.19175]\n",
      "4th training loss = 2.04445884765439\n",
      "4th training accuracy = [0.29025]\n",
      "5th training loss = 1.548676263573644\n",
      "5th training accuracy = [0.44125]\n",
      "6th training loss = 1.202538247524273\n",
      "6th training accuracy = [0.5755]\n",
      "7th training loss = 0.9675340239028206\n",
      "7th training accuracy = [0.704]\n",
      "8th training loss = 0.7265374437516712\n",
      "8th training accuracy = [0.7905]\n",
      "9th training loss = 0.5815561130311377\n",
      "9th training accuracy = [0.82925]\n",
      "10th training loss = 0.48799392038011385\n",
      "10th training accuracy = [0.8635]\n",
      "11th training loss = 0.4318121249591971\n",
      "11th training accuracy = [0.87875]\n",
      "12th training loss = 0.3890425238329508\n",
      "12th training accuracy = [0.8915]\n",
      "13th training loss = 0.35846128893400975\n",
      "13th training accuracy = [0.90025]\n",
      "14th training loss = 0.3285436973435765\n",
      "14th training accuracy = [0.91025]\n",
      "15th training loss = 0.3047833733996515\n",
      "15th training accuracy = [0.917]\n",
      "16th training loss = 0.2823534508731412\n",
      "16th training accuracy = [0.92225]\n",
      "17th training loss = 0.26306957925769714\n",
      "17th training accuracy = [0.923]\n",
      "18th training loss = 0.24229235304126712\n",
      "18th training accuracy = [0.92825]\n",
      "19th training loss = 0.22955270682878928\n",
      "19th training accuracy = [0.93575]\n",
      "20th training loss = 0.21207832724946166\n",
      "20th training accuracy = [0.94425]\n",
      "21th training loss = 0.1975423665296854\n",
      "21th training accuracy = [0.9465]\n",
      "22th training loss = 0.18764545524276602\n",
      "22th training accuracy = [0.9465]\n",
      "23th training loss = 0.1733784070078383\n",
      "23th training accuracy = [0.95]\n",
      "24th training loss = 0.16695627527345433\n",
      "24th training accuracy = [0.9535]\n",
      "25th training loss = 0.15450250270609373\n",
      "25th training accuracy = [0.9545]\n",
      "26th training loss = 0.144034690109004\n",
      "26th training accuracy = [0.9575]\n",
      "27th training loss = 0.13283453944416854\n",
      "27th training accuracy = [0.964]\n",
      "28th training loss = 0.12461950521628985\n",
      "28th training accuracy = [0.96775]\n",
      "29th training loss = 0.1178906642094921\n",
      "29th training accuracy = [0.969]\n",
      "30th training loss = 0.11204215661397045\n",
      "30th training accuracy = [0.97]\n",
      "31th training loss = 0.10330740975952915\n",
      "31th training accuracy = [0.974]\n",
      "32th training loss = 0.10020186546689931\n",
      "32th training accuracy = [0.97325]\n",
      "33th training loss = 0.09117951046828054\n",
      "33th training accuracy = [0.97625]\n",
      "34th training loss = 0.08299931527287847\n",
      "34th training accuracy = [0.97925]\n",
      "35th training loss = 0.07810257563868675\n",
      "35th training accuracy = [0.97975]\n",
      "36th training loss = 0.07396313694682671\n",
      "36th training accuracy = [0.981]\n",
      "37th training loss = 0.07103052665764516\n",
      "37th training accuracy = [0.983]\n",
      "38th training loss = 0.06370380966456402\n",
      "38th training accuracy = [0.9845]\n",
      "39th training loss = 0.05963425780677685\n",
      "39th training accuracy = [0.988]\n",
      "40th training loss = 0.05514348421827107\n",
      "40th training accuracy = [0.988]\n",
      "41th training loss = 0.05333651330709249\n",
      "41th training accuracy = [0.98875]\n",
      "42th training loss = 0.04843334867047754\n",
      "42th training accuracy = [0.9895]\n",
      "43th training loss = 0.04581213589493326\n",
      "43th training accuracy = [0.9925]\n",
      "44th training loss = 0.04237585829118496\n",
      "44th training accuracy = [0.9905]\n",
      "45th training loss = 0.0425569592548173\n",
      "45th training accuracy = [0.99175]\n",
      "46th training loss = 0.03939329636313575\n",
      "46th training accuracy = [0.9915]\n",
      "47th training loss = 0.03545439178955267\n",
      "47th training accuracy = [0.9935]\n",
      "48th training loss = 0.033627299727753746\n",
      "48th training accuracy = [0.994]\n",
      "49th training loss = 0.030625285162090808\n",
      "49th training accuracy = [0.994]\n",
      "50th training loss = 0.028833297378052417\n",
      "50th training accuracy = [0.995]\n",
      "51th training loss = 0.02662713378030335\n",
      "51th training accuracy = [0.9955]\n",
      "52th training loss = 0.024228103148537898\n",
      "52th training accuracy = [0.997]\n",
      "53th training loss = 0.02285590665383291\n",
      "53th training accuracy = [0.99675]\n",
      "54th training loss = 0.02245209491690945\n",
      "54th training accuracy = [0.99675]\n",
      "55th training loss = 0.020064162021150193\n",
      "55th training accuracy = [0.99775]\n",
      "56th training loss = 0.018574533833541695\n",
      "56th training accuracy = [0.99725]\n",
      "57th training loss = 0.017270456732794693\n",
      "57th training accuracy = [0.99775]\n",
      "58th training loss = 0.016776437547846502\n",
      "58th training accuracy = [0.9975]\n",
      "59th training loss = 0.015076480678267883\n",
      "59th training accuracy = [0.9975]\n",
      "60th training loss = 0.013675497327844916\n",
      "60th training accuracy = [0.9985]\n",
      "61th training loss = 0.01275041020977142\n",
      "61th training accuracy = [0.999]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "training_loss = {}\n",
    "training_accuracy = {}\n",
    "num_train_batches = mnist_data['Xtrain'].shape[0] / batchsize\n",
    "alpha = 0.02\n",
    "\n",
    "ind = 0\n",
    "for e in range(1, num_epochs + 1):\n",
    "    train_loader = DL.dataLoader(mnist_data['Xtrain'], mnist_data['ytrain'], batchsize)\n",
    "    training_loss.update({e: 0.0})\n",
    "    training_accuracy.update({e: 0.0})\n",
    "\n",
    "    for image, label in train_loader:\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model.forward(image)\n",
    "        # Backward pass\n",
    "        model.backward(image, label)\n",
    "        # Update\n",
    "        model.update(alpha)\n",
    "        # Calculate training loss and accuracy (Homework, see lecture)\n",
    "\n",
    "        training_loss[e] += loss.crossentropy(pred, label)\n",
    "        # print(NN.crossentropy(NN.softmax(pred), label_onehot))\n",
    "        #         print(NN.crossentropy(pred,label))\n",
    "        training_accuracy[e] += loss.ave_accuracy(pred, label)\n",
    "        # exit(1)\n",
    "    #     break\n",
    "\n",
    "    # At each epoch 'e', print training loss and accuracy (homework)\n",
    "    training_loss[e] = training_loss[e] / num_train_batches\n",
    "    training_accuracy[e] = training_accuracy[e] / num_train_batches\n",
    "    print(f'{e}th training loss = {training_loss[e]}\\n{e}th training accuracy = {training_accuracy[e]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(training_loss.keys()), list(training_loss.values()))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd0b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p38",
   "language": "python",
   "name": "pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
